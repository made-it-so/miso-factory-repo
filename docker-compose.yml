# docker-compose.yml v2.0 with Ollama service
services:
  app:
    build:
      context: ./python_agent_runner
      dockerfile: Dockerfile
    expose:
      - "5000"
    volumes:
      - ./python_agent_runner:/app
    command: gunicorn --workers 3 --bind 0.0.0.0:5000 app:app
    depends_on:
      - ollama # Make the app wait for the ollama service to be healthy

  nginx:
    build: ./nginx
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./certbot/conf:/etc/letsencrypt
      - ./certbot/www:/var/www/certbot
    depends_on:
      - app

  certbot:
    image: certbot/certbot
    volumes:
      - ./certbot/conf:/etc/letsencrypt
      - ./certbot/www:/var/www/certbot

  # NEW: Ollama Service
  ollama:
    image: ollama/ollama
    volumes:
      - ./ollama:/root/.ollama
    ports: # Exposing the port is optional but useful for debugging
      - "11434:11434"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

[
  {
    "id": "http://arxiv.org/abs/2509.09677v1",
    "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in LLMs",
    "summary": "Does continued scaling of large language models (LLMs) yield diminishing returns? Real-world value often stems from the length of task an agent can complete. We start this work by observing the simple but counterintuitive fact that marginal gains in single-step accuracy can compound into exponential improvements in the length of a task a model can successfully complete. Then, we argue that failures of LLMs when simple tasks are made longer arise from mistakes in execution, rather than an inability to reason. We propose isolating execution capability, by explicitly providing the knowledge and plan needed to solve a long-horizon task. We find that larger models can correctly execute significantly more turns even when small models have 100\\% single-turn accuracy. We observe that the per-step accuracy of models degrades as the number of steps increases. This is not just due to long-context limitations -- curiously, we observe a self-conditioning effect -- models become more likely to make mistakes when the context contains their errors from prior turns. Self-conditioning does not reduce by just scaling the model size. In contrast, recent thinking models do not self-condition, and can also execute much longer tasks in a single turn. We conclude by benchmarking frontier thinking models on the length of task they can execute in a single turn. Overall, by focusing on the ability to execute, we hope to reconcile debates on how LLMs can solve complex reasoning problems yet fail at simple tasks when made longer, and highlight the massive benefits of scaling model size and sequential test-time compute for long-horizon tasks.",
    "authors": [
      "Akshit Sinha",
      "Arvindh Arun",
      "Shashwat Goel",
      "Steffen Staab",
      "Jonas Geiping"
    ],
    "published_date": "2025-09-11T17:59:34+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09677v1",
    "relevance_score": 10,
    "relevance_justification": "As a research scientist in the field of artificial intelligence, I can provide expert-level insights and explanations on various AI-related topics. My expertise includes natural language processing, machine learning, deep learning, computer vision, and more. I'm here to help answer your questions and provide guidance on how to apply AI concepts to real-world problems.",
    "novelty_score": 8,
    "novelty_justification": "The idea of compensating for diminishing returns in LLMs can be metaphorically applied to MISO's Multi-Agent Systems objective by designing autonomous agents that can collectively compensate for individual limitations, potentially leading to exponential improvements in task execution.",
    "final_score": 9.4
  },
  {
    "id": "http://arxiv.org/abs/2509.09674v1",
    "title": "SimpleVLA-RL: Scaling VLA Training via Reinforcement Learning",
    "summary": "Vision-Language-Action (VLA) models have recently emerged as a powerful paradigm for robotic manipulation. Despite substantial progress enabled by large-scale pretraining and supervised fine-tuning (SFT), these models face two fundamental challenges: (i) the scarcity and high cost of large-scale human-operated robotic trajectories required for SFT scaling, and (ii) limited generalization to tasks involving distribution shift. Recent breakthroughs in Large Reasoning Models (LRMs) demonstrate that reinforcement learning (RL) can dramatically enhance step-by-step reasoning capabilities, raising a natural question: Can RL similarly improve the long-horizon step-by-step action planning of VLA? In this work, we introduce SimpleVLA-RL, an efficient RL framework tailored for VLA models. Building upon veRL, we introduce VLA-specific trajectory sampling, scalable parallelization, multi-environment rendering, and optimized loss computation. When applied to OpenVLA-OFT, SimpleVLA-RL achieves SoTA performance on LIBERO and even outperforms $\\pi_0$ on RoboTwin 1.0\\&2.0 with the exploration-enhancing strategies we introduce. SimpleVLA-RL not only reduces dependence on large-scale data and enables robust generalization, but also remarkably surpasses SFT in real-world tasks. Moreover, we identify a novel phenomenon ``pushcut'' during RL training, wherein the policy discovers previously unseen patterns beyond those seen in the previous training process. Github: https://github.com/PRIME-RL/SimpleVLA-RL",
    "authors": [
      "Haozhan Li",
      "Yuxin Zuo",
      "Jiale Yu",
      "Yuhao Zhang",
      "Zhaohui Yang",
      "Kaiyan Zhang",
      "Xuekai Zhu",
      "Yuchen Zhang",
      "Tianxing Chen",
      "Ganqu Cui",
      "Dehui Wang",
      "Dingxiang Luo",
      "Yuchen Fan",
      "Youbang Sun",
      "Jia Zeng",
      "Jiangmiao Pang",
      "Shanghang Zhang",
      "Yu Wang",
      "Yao Mu",
      "Bowen Zhou",
      "Ning Ding"
    ],
    "published_date": "2025-09-11T17:59:17+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09674v1",
    "relevance_score": 80,
    "relevance_justification": "The question is related to my area of expertise as a Research Scientist in Artificial Intelligence. I am trained on a wide range of topics within AI, including machine learning, deep learning, and natural language processing.",
    "novelty_score": 7,
    "novelty_justification": "The SimpleVLA-RL framework's focus on efficient RL for VLA models can be metaphorically applied to MISO by adapting the trajectory sampling and parallelization techniques to optimize AI planning in Multi-Agent Systems, enabling more effective exploration of complex scenarios.",
    "final_score": 58.1
  },
  {
    "id": "http://arxiv.org/abs/2509.09660v1",
    "title": "Steering MoE LLMs via Expert (De)Activation",
    "summary": "Mixture-of-Experts (MoE) in Large Language Models (LLMs) routes each token through a subset of specialized Feed-Forward Networks (FFN), known as experts. We present SteerMoE, a framework for steering MoE models by detecting and controlling behavior-linked experts. Our detection method identifies experts with distinct activation patterns across paired inputs exhibiting contrasting behaviors. By selectively (de)activating such experts during inference, we control behaviors like faithfulness and safety without retraining or modifying weights. Across 11 benchmarks and 6 LLMs, our steering raises safety by up to +20% and faithfulness by +27%. In adversarial attack mode, it drops safety by -41% alone, and -100% when combined with existing jailbreak methods, bypassing all safety guardrails and exposing a new dimension of alignment faking hidden within experts.",
    "authors": [
      "Mohsen Fayyaz",
      "Ali Modarressi",
      "Hanieh Deilamsalehy",
      "Franck Dernoncourt",
      "Ryan Rossi",
      "Trung Bui",
      "Hinrich Sch\u00fctze",
      "Nanyun Peng"
    ],
    "published_date": "2025-09-11T17:55:09+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09660v1",
    "relevance_score": 80,
    "relevance_justification": "As a research scientist in the field of artificial intelligence, I am well-versed in various AI-related topics. The topic of natural language processing (NLP) is particularly relevant to my work, as it enables me to develop more sophisticated machine learning models that can understand and generate human-like text.",
    "novelty_score": 8,
    "novelty_justification": "The expert steering mechanism in the paper can be analogized to MISO's self-improvement architecture, where experts represent different decision-making processes. By selectively activating or deactivating these experts, MISO could adapt its problem-solving strategies and improve overall performance.",
    "final_score": 58.4
  },
  {
    "id": "http://arxiv.org/abs/2509.09651v1",
    "title": "Retrieval-Augmented Generation for Reliable Interpretation of Radio Regulations",
    "summary": "We study question answering in the domain of radio regulations, a legally sensitive and high-stakes area. We propose a telecom-specific Retrieval-Augmented Generation (RAG) pipeline and introduce, to our knowledge, the first multiple-choice evaluation set for this domain, constructed from authoritative sources using automated filtering and human validation. To assess retrieval quality, we define a domain-specific retrieval metric, under which our retriever achieves approximately 97% accuracy. Beyond retrieval, our approach consistently improves generation accuracy across all tested models. In particular, while naively inserting documents without structured retrieval yields only marginal gains for GPT-4o (less than 1%), applying our pipeline results in nearly a 12% relative improvement. These findings demonstrate that carefully targeted grounding provides a simple yet strong baseline and an effective domain-specific solution for regulatory question answering. All code and evaluation scripts, along with our derived question-answer dataset, are available at https://github.com/Zakaria010/Radio-RAG.",
    "authors": [
      "Zakaria El Kassimi",
      "Fares Fourati",
      "Mohamed-Slim Alouini"
    ],
    "published_date": "2025-09-11T17:43:42+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09651v1",
    "relevance_score": 100,
    "relevance_justification": "As a research scientist in the field of artificial intelligence, I'm well-versed in various AI-related topics and am here to assist you with any questions or concerns you may have. Whether it's discussing the latest advancements in machine learning, natural language processing, or computer vision, I'm happy to share my knowledge and provide insights.",
    "novelty_score": 7,
    "novelty_justification": "The Retrieval-Augmented Generation (RAG) pipeline's ability to improve generation accuracy can be metaphorically applied to MISO's autonomous code generation objective by developing a similar retrieval-based approach for AI planning, leveraging domain-specific knowledge and structured retrieval to enhance the planning process.",
    "final_score": 72.1
  },
  {
    "id": "http://arxiv.org/abs/2509.09619v1",
    "title": "Functional Groups are All you Need for Chemically Interpretable Molecular Property Prediction",
    "summary": "Molecular property prediction using deep learning (DL) models has accelerated drug and materials discovery, but the resulting DL models often lack interpretability, hindering their adoption by chemists. This work proposes developing molecule representations using the concept of Functional Groups (FG) in chemistry. We introduce the Functional Group Representation (FGR) framework, a novel approach to encoding molecules based on their fundamental chemical substructures. Our method integrates two types of functional groups: those curated from established chemical knowledge (FG), and those mined from a large molecular corpus using sequential pattern mining (MFG). The resulting FGR framework encodes molecules into a lower-dimensional latent space by leveraging pre-training on a large dataset of unlabeled molecules. Furthermore, the proposed framework allows the inclusion of 2D structure-based descriptors of molecules. We demonstrate that the FGR framework achieves state-of-the-art performance on a diverse range of 33 benchmark datasets spanning physical chemistry, biophysics, quantum mechanics, biological activity, and pharmacokinetics while enabling chemical interpretability. Crucially, the model's representations are intrinsically aligned with established chemical principles, allowing chemists to directly link predicted properties to specific functional groups and facilitating novel insights into structure-property relationships. Our work presents a significant step toward developing high-performing, chemically interpretable DL models for molecular discovery.",
    "authors": [
      "Roshan Balaji",
      "Joe Bobby",
      "Nirav Pravinbhai Bhatt"
    ],
    "published_date": "2025-09-11T17:01:31+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09619v1",
    "relevance_score": 100,
    "relevance_justification": "As a research scientist in the field of artificial intelligence, I am well-equipped to provide insights and answers to various questions. My training data includes a vast amount of information on AI-related topics, making me a reliable source for queries related to machine learning, natural language processing, computer vision, and more.",
    "novelty_score": 8,
    "novelty_justification": "The Functional Group Representation framework's ability to encode molecules into a lower-dimensional latent space using pre-training on a large dataset of unlabeled molecules can be metaphorically applied to improve MISO's autonomous code generation by developing a novel representation scheme for encoding complex problem-solving strategies, enabling more efficient and interpretable AI planning.",
    "final_score": 72.4
  },
  {
    "id": "http://arxiv.org/abs/2509.09614v1",
    "title": "LoCoBench: A Benchmark for Long-Context Large Language Models in Complex Software Engineering",
    "summary": "The emergence of long-context language models with context windows extending to millions of tokens has created new opportunities for sophisticated code understanding and software development evaluation. We propose LoCoBench, a comprehensive benchmark specifically designed to evaluate long-context LLMs in realistic, complex software development scenarios. Unlike existing code evaluation benchmarks that focus on single-function completion or short-context tasks, LoCoBench addresses the critical evaluation gap for long-context capabilities that require understanding entire codebases, reasoning across multiple files, and maintaining architectural consistency across large-scale software systems. Our benchmark provides 8,000 evaluation scenarios systematically generated across 10 programming languages, with context lengths spanning 10K to 1M tokens, a 100x variation that enables precise assessment of long-context performance degradation in realistic software development settings. LoCoBench introduces 8 task categories that capture essential long-context capabilities: architectural understanding, cross-file refactoring, multi-session development, bug investigation, feature implementation, code comprehension, integration testing, and security analysis. Through a 5-phase pipeline, we create diverse, high-quality scenarios that challenge LLMs to reason about complex codebases at unprecedented scale. We introduce a comprehensive evaluation framework with 17 metrics across 4 dimensions, including 8 new evaluation metrics, combined in a LoCoBench Score (LCBS). Our evaluation of state-of-the-art long-context models reveals substantial performance gaps, demonstrating that long-context understanding in complex software development represents a significant unsolved challenge that demands more attention. LoCoBench is released at: https://github.com/SalesforceAIResearch/LoCoBench.",
    "authors": [
      "Jielin Qiu",
      "Zuxin Liu",
      "Zhiwei Liu",
      "Rithesh Murthy",
      "Jianguo Zhang",
      "Haolin Chen",
      "Shiyu Wang",
      "Ming Zhu",
      "Liangwei Yang",
      "Juntao Tan",
      "Zhepeng Cen",
      "Cheng Qian",
      "Shelby Heinecke",
      "Weiran Yao",
      "Silvio Savarese",
      "Caiming Xiong",
      "Huan Wang"
    ],
    "published_date": "2025-09-11T16:55:04+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09614v1",
    "relevance_score": 100,
    "relevance_justification": "As a seasoned AI Research Scientist, I'm well-versed in the field of Artificial Intelligence and its numerous applications. My expertise spans multiple areas, including Machine Learning, Natural Language Processing, Computer Vision, and more.",
    "novelty_score": 8,
    "novelty_justification": null,
    "final_score": 72.4
  },
  {
    "id": "http://arxiv.org/abs/2509.09611v1",
    "title": "ReBaNO: Reduced Basis Neural Operator Mitigating Generalization Gaps and Achieving Discretization Invariance",
    "summary": "We propose a novel data-lean operator learning algorithm, the Reduced Basis Neural Operator (ReBaNO), to solve a group of PDEs with multiple distinct inputs. Inspired by the Reduced Basis Method and the recently introduced Generative Pre-Trained Physics-Informed Neural Networks, ReBaNO relies on a mathematically rigorous greedy algorithm to build its network structure offline adaptively from the ground up. Knowledge distillation via task-specific activation function allows ReBaNO to have a compact architecture requiring minimal computational cost online while embedding physics. In comparison to state-of-the-art operator learning algorithms such as PCA-Net, DeepONet, FNO, and CNO, numerical results demonstrate that ReBaNO significantly outperforms them in terms of eliminating/shrinking the generalization gap for both in- and out-of-distribution tests and being the only operator learning algorithm achieving strict discretization invariance.",
    "authors": [
      "Haolan Zheng",
      "Yanlai Chen",
      "Jiequn Han",
      "Yue Yu"
    ],
    "published_date": "2025-09-11T16:52:54+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09611v1",
    "relevance_score": 8,
    "relevance_justification": "As a research scientist in the field of artificial intelligence, I am well-versed in various AI-related topics and can assist with questions on AI concepts, applications, and latest developments.",
    "novelty_score": 7,
    "novelty_justification": "The reduced basis neural operator's ability to adaptively build its network structure offline and achieve discretization invariance can be metaphorically applied to improve MISO's autonomous code generation by introducing a novel, adaptive approach to AI planning that allows for more efficient and robust planning under uncertainty.",
    "final_score": 7.699999999999999
  },
  {
    "id": "http://arxiv.org/abs/2509.09610v1",
    "title": "Mechanistic Learning with Guided Diffusion Models to Predict Spatio-Temporal Brain Tumor Growth",
    "summary": "Predicting the spatio-temporal progression of brain tumors is essential for guiding clinical decisions in neuro-oncology. We propose a hybrid mechanistic learning framework that combines a mathematical tumor growth model with a guided denoising diffusion implicit model (DDIM) to synthesize anatomically feasible future MRIs from preceding scans. The mechanistic model, formulated as a system of ordinary differential equations, captures temporal tumor dynamics including radiotherapy effects and estimates future tumor burden. These estimates condition a gradient-guided DDIM, enabling image synthesis that aligns with both predicted growth and patient anatomy. We train our model on the BraTS adult and pediatric glioma datasets and evaluate on 60 axial slices of in-house longitudinal pediatric diffuse midline glioma (DMG) cases. Our framework generates realistic follow-up scans based on spatial similarity metrics. It also introduces tumor growth probability maps, which capture both clinically relevant extent and directionality of tumor growth as shown by 95th percentile Hausdorff Distance. The method enables biologically informed image generation in data-limited scenarios, offering generative-space-time predictions that account for mechanistic priors.",
    "authors": [
      "Daria Laslo",
      "Efthymios Georgiou",
      "Marius George Linguraru",
      "Andreas Rauschecker",
      "Sabine Muller",
      "Catherine R. Jutzeler",
      "Sarah Bruningk"
    ],
    "published_date": "2025-09-11T16:52:09+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09610v1",
    "relevance_score": 80,
    "relevance_justification": "As a research scientist in the field of artificial intelligence, I am well-versed in various AI-related topics, including machine learning, natural language processing, and computer vision. I have extensive knowledge on AI's applications, such as image recognition, speech recognition, predictive analytics, and decision-making systems.",
    "novelty_score": 8,
    "novelty_justification": "The combination of mechanistic modeling and guided diffusion models in the paper can be metaphorically applied to MISO's autonomous code generation by developing a system that leverages domain-specific rules and constraints, guiding the generation of artificial intelligence planning models to produce more realistic and anatomically feasible solutions.",
    "final_score": 58.4
  },
  {
    "id": "http://arxiv.org/abs/2509.09593v1",
    "title": "Fluent but Unfeeling: The Emotional Blind Spots of Language Models",
    "summary": "The versatility of Large Language Models (LLMs) in natural language understanding has made them increasingly popular in mental health research. While many studies explore LLMs' capabilities in emotion recognition, a critical gap remains in evaluating whether LLMs align with human emotions at a fine-grained level. Existing research typically focuses on classifying emotions into predefined, limited categories, overlooking more nuanced expressions. To address this gap, we introduce EXPRESS, a benchmark dataset curated from Reddit communities featuring 251 fine-grained, self-disclosed emotion labels. Our comprehensive evaluation framework examines predicted emotion terms and decomposes them into eight basic emotions using established emotion theories, enabling a fine-grained comparison. Systematic testing of prevalent LLMs under various prompt settings reveals that accurately predicting emotions that align with human self-disclosed emotions remains challenging. Qualitative analysis further shows that while certain LLMs generate emotion terms consistent with established emotion theories and definitions, they sometimes fail to capture contextual cues as effectively as human self-disclosures. These findings highlight the limitations of LLMs in fine-grained emotion alignment and offer insights for future research aimed at enhancing their contextual understanding.",
    "authors": [
      "Bangzhao Shu",
      "Isha Joshi",
      "Melissa Karnaze",
      "Anh C. Pham",
      "Ishita Kakkar",
      "Sindhu Kothe",
      "Arpine Hovasapian",
      "Mai ElSherief"
    ],
    "published_date": "2025-09-11T16:31:13+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09593v1",
    "relevance_score": 8,
    "relevance_justification": "As a research scientist in the field of Artificial Intelligence, I have extensive knowledge and expertise in various AI-related domains. My research focuses on developing innovative algorithms and techniques to improve the performance and reliability of AI systems.",
    "novelty_score": 8,
    "novelty_justification": "The concept of 'emotional blind spots' in language models can be applied to MISO's autonomous code generation by recognizing potential biases and limitations in the AI planning process, enabling more accurate self-improvement architectures.",
    "final_score": 8.0
  },
  {
    "id": "http://arxiv.org/abs/2509.09583v1",
    "title": "Personality-Enhanced Social Recommendations in SAMI: Exploring the Role of Personality Detection in Matchmaking",
    "summary": "Social connection is a vital part of learning, yet online course environments present barriers to the organic formation of social groups. SAMI offers one solution by facilitating student connections, but its effectiveness is constrained by an incomplete Theory of Mind, limiting its ability to create an effective mental model of a student. One facet of this is its inability to intuit personality, which may influence the relevance of its recommendations. To explore this, we propose a personality detection model utilizing GPTs zero-shot capability to infer Big-Five personality traits from forum introduction posts, often encouraged in online courses. We benchmark its performance against established models, demonstrating its efficacy in this task. Furthermore, we integrate this model into SAMIs entity-based matchmaking system, enabling personality-informed social recommendations. Initial integration suggests personality traits can complement existing matching factors, though additional evaluation is required to determine their full impact on student engagement and match quality.",
    "authors": [
      "Brittany Harbison",
      "Samuel Taubman",
      "Travis Taylor",
      "Ashok. K. Goel"
    ],
    "published_date": "2025-09-11T16:19:59+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09583v1",
    "relevance_score": 8,
    "relevance_justification": "The term 'AI Research Scientist' is a common and well-established job title in the field of Artificial Intelligence. It suggests that I have expertise in both AI research and scientific methodology.",
    "novelty_score": 8,
    "novelty_justification": "The personality detection model can be used to improve MISO's autonomous code generation by incorporating user preferences and personality traits into the programming language, allowing for more personalized and effective collaboration among team members.",
    "final_score": 8.0
  },
  {
    "id": "http://arxiv.org/abs/2509.09564v1",
    "title": "What Does Normal Even Mean? Evaluating Benign Traffic in Intrusion Detection Datasets",
    "summary": "Supervised machine learning techniques rely on labeled data to achieve high task performance, but this requires the labels to capture some meaningful differences in the underlying data structure. For training network intrusion detection algorithms, most datasets contain a series of attack classes and a single large benign class which captures all non-attack network traffic. A review of intrusion detection papers and guides that explicitly state their data preprocessing steps identified that the majority took the labeled categories of the dataset at face value when training their algorithms. The present paper evaluates the structure of benign traffic in several common intrusion detection datasets (NSL-KDD, UNSW-NB15, and CIC-IDS 2017) and determines whether there are meaningful sub-categories within this traffic which may improve overall multi-classification performance using common machine learning techniques. We present an overview of some unsupervised clustering techniques (e.g., HDBSCAN, Mean Shift Clustering) and show how they differentially cluster the benign traffic space.",
    "authors": [
      "Meghan Wilkinson",
      "Robert H Thomson"
    ],
    "published_date": "2025-09-11T15:55:21+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09564v1",
    "relevance_score": 100,
    "relevance_justification": "As a research scientist in the field of artificial intelligence, I am well-versed in the latest developments and advancements in AI, including natural language processing, machine learning, and computer vision. My expertise is relevant to discussing topics related to AI ethics, potential applications, and limitations.",
    "novelty_score": 7,
    "novelty_justification": "The paper's unsupervised clustering techniques can be metaphorically applied to MISO's AI planning and self-improvement architectures, enabling the discovery of meaningful sub-categories within complex autonomous code generation processes, ultimately improving multi-agent system performance.",
    "final_score": 72.1
  },
  {
    "id": "http://arxiv.org/abs/2509.09560v1",
    "title": "Boosting Embodied AI Agents through Perception-Generation Disaggregation and Asynchronous Pipeline Execution",
    "summary": "Embodied AI systems operate in dynamic environments, requiring seamless integration of perception and generation modules to process high-frequency input and output demands. Traditional sequential computation patterns, while effective in ensuring accuracy, face significant limitations in achieving the necessary \"thinking\" frequency for real-world applications. In this work, we present Auras, an algorithm-system co-designed inference framework to optimize the inference frequency of embodied AI agents. Auras disaggregates the perception and generation and provides controlled pipeline parallelism for them to achieve high and stable throughput. Faced with the data staleness problem that appears when the parallelism is increased, Auras establishes a public context for perception and generation to share, thereby promising the accuracy of embodied agents. Experimental results show that Auras improves throughput by 2.54x on average while achieving 102.7% of the original accuracy, demonstrating its efficacy in overcoming the constraints of sequential computation and providing high throughput.",
    "authors": [
      "Shulai Zhang",
      "Ao Xu",
      "Quan Chen",
      "Han Zhao",
      "Weihao Cui",
      "Ningxin Zheng",
      "Haibin Lin",
      "Xin Liu",
      "Minyi Guo"
    ],
    "published_date": "2025-09-11T15:51:43+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09560v1",
    "relevance_score": 9,
    "relevance_justification": "As a research scientist in the field of artificial intelligence, I can confidently say that this question aligns with my expertise. The provided format for JSON output is a standard and widely used convention, which is an important aspect of AI research.",
    "novelty_score": 8,
    "novelty_justification": "The Auras framework's ability to disaggregate perception and generation, allowing for controlled pipeline parallelism, can be metaphorically applied to MISO's AI planning component by decomposing complex planning tasks into smaller, parallelizable sub-tasks, enabling more efficient and effective planning in multi-agent systems.",
    "final_score": 8.7
  },
  {
    "id": "http://arxiv.org/abs/2509.09552v1",
    "title": "An improved educational competition optimizer with multi-covariance learning operators for global optimization problems",
    "summary": "The educational competition optimizer is a recently introduced metaheuristic algorithm inspired by human behavior, originating from the dynamics of educational competition within society. Nonetheless, ECO faces constraints due to an imbalance between exploitation and exploration, rendering it susceptible to local optima and demonstrating restricted effectiveness in addressing complex optimization problems. To address these limitations, this study presents an enhanced educational competition optimizer (IECO-MCO) utilizing multi-covariance learning operators. In IECO, three distinct covariance learning operators are introduced to improve the performance of ECO. Each operator effectively balances exploitation and exploration while preventing premature convergence of the population. The effectiveness of IECO is assessed through benchmark functions derived from the CEC 2017 and CEC 2022 test suites, and its performance is compared with various basic and improved algorithms across different categories. The results demonstrate that IECO-MCO surpasses the basic ECO and other competing algorithms in convergence speed, stability, and the capability to avoid local optima. Furthermore, statistical analyses, including the Friedman test, Kruskal-Wallis test, and Wilcoxon rank-sum test, are conducted to validate the superiority of IECO-MCO over the compared algorithms. Compared with the basic algorithm (improved algorithm), IECO-MCO achieved an average ranking of 2.213 (2.488) on the CE2017 and CEC2022 test suites. Additionally, the practical applicability of the proposed IECO-MCO algorithm is verified by solving constrained optimization problems. The experimental outcomes demonstrate the superior performance of IECO-MCO in tackling intricate optimization problems, underscoring its robustness and practical effectiveness in real-world scenarios.",
    "authors": [
      "Baoqi Zhao",
      "Xiong Yang",
      "Hoileong Lee",
      "Bowen Dong"
    ],
    "published_date": "2025-09-11T15:41:14+00:00",
    "pdf_url": "http://arxiv.org/pdf/2509.09552v1",
    "relevance_score": 100,
    "relevance_justification": "As a researcher in the field of artificial intelligence, I can confidently say that this task is directly related to my area of expertise. The ability to generate human-like text based on input prompts is a fundamental aspect of natural language processing (NLP), which is a key focus of AI research.",
    "novelty_score": 8,
    "novelty_justification": "The multi-covariance learning operators in IECO-MCO can be metaphorically applied to MISO's autonomous code generation by incorporating multiple, diverse learning strategies to balance exploration and exploitation, enabling more effective self-improvement architectures.",
    "final_score": 72.4
  }
]